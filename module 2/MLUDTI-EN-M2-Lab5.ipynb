{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc91120c",
   "metadata": {},
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 2, Lab 5: Finetuning BERT\n",
    "\n",
    "\n",
    "BERT stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. To learn how BERT works, let's fine-tune the __BERT__ model to classify product reviews. You will use a new library called __transformers__ to download a pre-trained BERT model. \n",
    "\n",
    "You will learn:\n",
    "\n",
    "- How to load and format the dataset\n",
    "- How to load the pre-trained model\n",
    "- How to train and test the model\n",
    "\n",
    "__BERT and its variants use more resources than the other models you have used so far. This may cause your instance to run out of memory. If that happens:__\n",
    "\n",
    "- Restart the kernel (Kernel->Restart from the top menu)\n",
    "- Reduce the batch size \n",
    "- Then re-run the code\n",
    "\n",
    "\n",
    "\n",
    "__Note__: In this walkthrough, you will use a light version of the original BERT implementation called __\"DistilBert\"__. You can checkout [the paper](https://arxiv.org/pdf/1910.01108.pdf) about it for more details. \n",
    "\n",
    "---\n",
    "This lab uses a dataset derived from a small sample of Amazon product reviews. \n",
    "\n",
    "__Review dataset schema:__\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log\\_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc56e8-353d-4511-a225-eb2521cf946a",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. [Reading and formatting the dataset](#Reading-and-formatting-the-dataset)\n",
    "1. [Loading the pre-trained model](#Loading-the-pre-trained-model)\n",
    "1. [Training and testing the model](#Training-and-testing-the-model)\n",
    "1. [Getting predictions on the test data](#Getting-predictions-on-the-test-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704d3332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378cb560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.39.3\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.39.3) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.8.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.3) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.39.3) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.39.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.39.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2023.7.22)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.39.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.39.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3c795",
   "metadata": {},
   "source": [
    "## Reading and formatting the dataset\n",
    "\n",
    "First, you need to read in the product review dataset and prepare it for the BERT model. To keep the training time down, you will only use the first 2000 data points from the dataset. If you want to improve your model after you understand how to train, you can use more data to train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f782fdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import system library and append path\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "# Setting tokenizer parallelism to false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Import utility functions that provide answers to challenges\n",
    "from MLUDTI_EN_M2_Lab5_quiz_questions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dfe2d",
   "metadata": {},
   "source": [
    "Read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af83f37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2a4ec",
   "metadata": {},
   "source": [
    "Print the dataset information to see the field types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc624ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   ID          56000 non-null  int64  \n",
      " 1   reviewText  55989 non-null  object \n",
      " 2   summary     55987 non-null  object \n",
      " 3   verified    56000 non-null  bool   \n",
      " 4   time        56000 non-null  int64  \n",
      " 5   log_votes   56000 non-null  float64\n",
      " 6   isPositive  56000 non-null  int64  \n",
      "dtypes: bool(1), float64(1), int64(3), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a4c80",
   "metadata": {},
   "source": [
    "You do not need any of the rows that do not have __reviewText__, so drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f993d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efeb89-d57e-41b5-afa9-0d4fea1bf745",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the question below to test your understanding of epochs and learning rate.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caafb228-0fb2-4d35-8b95-2bed396a0714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e21c",
   "metadata": {},
   "source": [
    "BERT requires a lot of compute power for large datasets. To reduce the amount of time it takes to train the model, you will only use the first 2,000 data points for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342b01ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2d8b9",
   "metadata": {},
   "source": [
    "Now split the dataset into training and validation data sets, keeping 10% of the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bbdd5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"reviewText\"].tolist(),\n",
    "    df[\"isPositive\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    "    stratify = df[\"isPositive\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f7323",
   "metadata": {},
   "source": [
    "You need to tokenize the data. To do this, use a special tokenizer built for the DistilBERT model to tokenize the training and validation texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5ff8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cba272d6654920b0e0feb20df5d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d0f81ccaa24198b423b40d32e8462a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6203ed6e924e758ba0312ae08944bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03556a908e0e47418e1d9ec33f73082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92881b",
   "metadata": {},
   "source": [
    "Create a new `ReviewDataset` class to use with the BERT model. Later, you use the training and validation encoding-label pairs with this new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332a8e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = ReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbfae6",
   "metadata": {},
   "source": [
    "## Loading the pre-trained model\n",
    "\n",
    "Now, you need to load the model. When you do this, several warnings will print that are related to the last classification layer of BERT where you are using a randomly initialized layer. You can ignore the warnings as they are not relevant to the type of training you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a42e0ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1598ab0b9a964663b07beea45ac69d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                            num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa228cc",
   "metadata": {},
   "source": [
    "The last step is to freeze all weights until the very last classification layer in the BERT model. This helps accelerate the training process. Training the weights of the whole network (66 million weights) takes a long time. Additionally, 2000 data points would not be enough for that task. Instead, the code below freezes all the weights until the last classification layer. This means only a small portion of the weights gets updated (rest stays the same). This is a common practice with large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5468e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the encoder weights until the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015dfe3",
   "metadata": {},
   "source": [
    "## Training and testing the model\n",
    "\n",
    "Now that your data is ready and you have configured your model, its time to start the fine-tuning process. This code will take __a long time__ (30+ minutes) to complete with large datasets, that is why you are running it on a subset of the full review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a0f41",
   "metadata": {},
   "source": [
    "First, define the accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62a035a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, label):\n",
    "    \"\"\"Calculate the accuracy of the trained network. \n",
    "    output: (batch_size, num_output) float32 tensor\n",
    "    label: (batch_size, ) int32 tensor \"\"\"\n",
    "    \n",
    "    return (output.argmax(axis=1) == label.float()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca5e8a",
   "metadata": {},
   "source": [
    "Now you need to create the tranining and validation loop. This loop will be similar to the previous train/validation loops, however there are a few extra parameters needed due to the transformer architecture. \n",
    "\n",
    "You need to use the `attention_mask` and get the loss from the output of the model with `loss = output[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e98aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.651, train acc 0.618, val loss 0.630, val acc 0.620, seconds  56.288 \n",
      "Epoch 2: train loss 0.623, train acc 0.631, val loss 0.604, val acc 0.620, seconds  30.024 \n",
      "Epoch 3: train loss 0.600, train acc 0.659, val loss 0.578, val acc 0.700, seconds  30.097 \n",
      "Epoch 4: train loss 0.574, train acc 0.706, val loss 0.554, val acc 0.760, seconds  30.189 \n",
      "Epoch 5: train loss 0.558, train acc 0.730, val loss 0.529, val acc 0.700, seconds  30.241 \n",
      "Epoch 6: train loss 0.530, train acc 0.759, val loss 0.509, val acc 0.800, seconds  30.266 \n",
      "Epoch 7: train loss 0.512, train acc 0.765, val loss 0.484, val acc 0.810, seconds  30.265 \n",
      "Epoch 8: train loss 0.488, train acc 0.796, val loss 0.461, val acc 0.815, seconds  30.293 \n",
      "Epoch 9: train loss 0.474, train acc 0.795, val loss 0.446, val acc 0.790, seconds  30.285 \n",
      "Epoch 10: train loss 0.458, train acc 0.806, val loss 0.423, val acc 0.845, seconds  30.289 \n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,\n",
    "                          batch_size=16, drop_last=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8,\n",
    "                               drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, val_loss, train_acc, valid_acc = 0., 0., 0., 0.\n",
    "    \n",
    "    start = time.time()\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    for batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Put data, label and attention mask to the correct device\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Make forward pass\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        \n",
    "        # Calculate the loss (this comes from the output)\n",
    "        loss = output[0]\n",
    "        # Make backwards pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        # Accumulate training accuracy and loss\n",
    "        train_acc += calculate_accuracy(output.logits, label).item()\n",
    "        train_loss += loss.item()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop:\n",
    "    # This loop tests the trained network on validation dataset\n",
    "    # No weight updates here\n",
    "    # torch.no_grad() reduces memory usage when not training the network\n",
    "    model.eval() # Activate evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            data = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch[\"labels\"].to(device)\n",
    "            # Make forward pass with the trained model so far\n",
    "            output = model(data, attention_mask=attention_mask, labels=label)\n",
    "            # Accumulate validation accuracy and loss\n",
    "            valid_acc += calculate_accuracy(output.logits, label).item()\n",
    "            val_loss += output[0].item()\n",
    "        \n",
    "    # Take averages\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    val_loss /= len(validation_loader)\n",
    "    valid_acc /= len(validation_loader)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Epoch %d: train loss %.3f, train acc %.3f, val loss %.3f, val acc %.3f, seconds % .3f \" % (\n",
    "        epoch+1, train_loss, train_acc, val_loss, valid_acc, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7ce78",
   "metadata": {},
   "source": [
    "### Looking at what's going on\n",
    "\n",
    "The fine-tuned BERT model is able to correctly classify the sentiment of the most of the records in the validation set. Let's observe in more detail how the sentences are tokenized and encoded. You can do this by picking one sentence as example to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195dd23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: An excellent resource for all scanner owners.  Seems to be up to date and all inclusive.  I highly recommend this product!\n",
      "Encoded Sentence: [101, 2019, 6581, 7692, 2005, 2035, 26221, 5608, 1012, 3849, 2000, 2022, 2039, 2000, 3058, 1998, 2035, 18678, 1012, 1045, 3811, 16755, 2023, 4031, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "st = val_texts[19]\n",
    "print(f\"Sentence: {st}\")\n",
    "tok = tokenizer(st, truncation=True, padding=True)\n",
    "print(f\"Encoded Sentence: {tok['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5359be5",
   "metadata": {},
   "source": [
    "Print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c78bc31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mapped vocabulary is stored in tokenizer.vocab\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738febb",
   "metadata": {},
   "source": [
    "Use the encoded sentence with the tokenizer to recover the original sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8ae225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'an', 'excellent', 'resource', 'for', 'all', 'scanner', 'owners', '.', 'seems', 'to', 'be', 'up', 'to', 'date', 'and', 'all', 'inclusive', '.', 'i', 'highly', 'recommend', 'this', 'product', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Methods convert_ids_to_tokens and convert_tokens_to_ids allow to see how sentences are tokenized\n",
    "print(tokenizer.convert_ids_to_tokens(tok[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89346761",
   "metadata": {},
   "source": [
    "## Getting predictions on the test data\n",
    "\n",
    "After the model is trained, you can focus on getting test data to make predictions with. Do this by:\n",
    "- Reading and format the test dataset\n",
    "- Passing the test data to your trained model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd36b78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33276</td>\n",
       "      <td>I've been using greeting card software for wel...</td>\n",
       "      <td>Absolutely awful.</td>\n",
       "      <td>False</td>\n",
       "      <td>1300233600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20859</td>\n",
       "      <td>This version worked well for me, have upgraded...</td>\n",
       "      <td>Good for virtual machine on a mac</td>\n",
       "      <td>True</td>\n",
       "      <td>1448755200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63500</td>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1456963200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4950</td>\n",
       "      <td>I can assure you that any five star review was...</td>\n",
       "      <td>SCAM</td>\n",
       "      <td>False</td>\n",
       "      <td>1400803200</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26509</td>\n",
       "      <td>Overall the product really seems the same but ...</td>\n",
       "      <td>Has potential but many glitches and really the...</td>\n",
       "      <td>False</td>\n",
       "      <td>1419206400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  33276  I've been using greeting card software for wel...   \n",
       "1  20859  This version worked well for me, have upgraded...   \n",
       "2  63500                                             Great!   \n",
       "3   4950  I can assure you that any five star review was...   \n",
       "4  26509  Overall the product really seems the same but ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Absolutely awful.     False  1300233600   \n",
       "1                  Good for virtual machine on a mac      True  1448755200   \n",
       "2                                         Five Stars      True  1456963200   \n",
       "3                                               SCAM     False  1400803200   \n",
       "4  Has potential but many glitches and really the...     False  1419206400   \n",
       "\n",
       "   log_votes  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3   2.197225  \n",
       "4   0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the test data (It doesn't have the isPositive label)\n",
    "df_test = pd.read_csv(\"data/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6477c10",
   "metadata": {},
   "source": [
    "Just as before, drop the rows that don't have the __reviewText__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb479bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588e159",
   "metadata": {},
   "source": [
    "Making predictions will also take a long time with this model. To get results quickly, start by only making predictions with 15 datapoints from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a35daf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts = df_test[\"reviewText\"].tolist()[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19e9cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts,\n",
    "                           truncation=True,\n",
    "                           padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640fe83",
   "metadata": {},
   "source": [
    "Create labels for the test dataset to pass zeros using `[0]*len(test_texts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e8ba88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb780",
   "metadata": {},
   "source": [
    "Then, create a dataloader for the test set and record the corresponding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4012e571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        predictions = torch.argmax(output.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202c1f0",
   "metadata": {},
   "source": [
    "Finally, pick an example sentence and examine the prediction. Does the prediction look correct? \n",
    "\n",
    "Remember \n",
    "\n",
    "- 1->positive class \n",
    "- 0->negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f674a041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I have been using this product for five or six years.  This purchase was my annual subscription renewal.  It has the features that I need, and seems to protect the three PC's that we have while using the internet.  I will be looking at the mobile apps for a tablet and smart phone.\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "k = 13\n",
    "print(f'Text: {test_texts[k]}')\n",
    "print(f'Prediction: {test_predictions[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c89a3",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center; margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">You trained the model for 10 epochs. Would you get better results from the validation dataset if the model trained longer?</p> <br>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Make a note of your last <code> Val_loss </code> result.</p> \n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Then, in the <a href=\"#Training-and-testing-the-model\">Training and testing the model</a> section, change the <code> num_epochs </code> parameter to <code>20</code>.</p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Finally, re-run the code blocks to load the pre-trained model, and train your model.</p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Did <code>Val_loss</code> improve?</p>\n",
    "    </ol>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d443b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.442, train acc 0.805, val loss 0.410, val acc 0.850, seconds  30.284 \n",
      "Epoch 2: train loss 0.437, train acc 0.804, val loss 0.395, val acc 0.845, seconds  30.288 \n",
      "Epoch 3: train loss 0.420, train acc 0.818, val loss 0.386, val acc 0.840, seconds  30.272 \n",
      "Epoch 4: train loss 0.414, train acc 0.819, val loss 0.372, val acc 0.865, seconds  30.280 \n",
      "Epoch 5: train loss 0.407, train acc 0.828, val loss 0.364, val acc 0.875, seconds  30.283 \n",
      "Epoch 6: train loss 0.402, train acc 0.825, val loss 0.357, val acc 0.870, seconds  30.276 \n",
      "Epoch 7: train loss 0.393, train acc 0.830, val loss 0.351, val acc 0.870, seconds  30.261 \n",
      "Epoch 8: train loss 0.389, train acc 0.825, val loss 0.343, val acc 0.880, seconds  30.277 \n",
      "Epoch 9: train loss 0.387, train acc 0.838, val loss 0.338, val acc 0.880, seconds  30.276 \n",
      "Epoch 10: train loss 0.378, train acc 0.835, val loss 0.335, val acc 0.870, seconds  30.271 \n",
      "Epoch 11: train loss 0.374, train acc 0.836, val loss 0.336, val acc 0.895, seconds  30.282 \n",
      "Epoch 12: train loss 0.374, train acc 0.831, val loss 0.324, val acc 0.875, seconds  30.295 \n",
      "Epoch 13: train loss 0.369, train acc 0.842, val loss 0.358, val acc 0.860, seconds  30.281 \n",
      "Epoch 14: train loss 0.365, train acc 0.836, val loss 0.318, val acc 0.880, seconds  30.279 \n",
      "Epoch 15: train loss 0.368, train acc 0.837, val loss 0.325, val acc 0.895, seconds  30.281 \n",
      "Epoch 16: train loss 0.358, train acc 0.839, val loss 0.312, val acc 0.880, seconds  30.288 \n",
      "Epoch 17: train loss 0.357, train acc 0.854, val loss 0.308, val acc 0.880, seconds  30.272 \n",
      "Epoch 18: train loss 0.354, train acc 0.841, val loss 0.308, val acc 0.885, seconds  30.278 \n",
      "Epoch 19: train loss 0.352, train acc 0.841, val loss 0.306, val acc 0.885, seconds  30.263 \n",
      "Epoch 20: train loss 0.355, train acc 0.843, val loss 0.304, val acc 0.900, seconds  30.283 \n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 20 # change number of epochs to 20\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,\n",
    "                          batch_size=16, drop_last=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8,\n",
    "                               drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, val_loss, train_acc, valid_acc = 0., 0., 0., 0.\n",
    "    \n",
    "    start = time.time()\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    for batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Put data, label and attention mask to the correct device\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Make forward pass\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        \n",
    "        # Calculate the loss (this comes from the output)\n",
    "        loss = output[0]\n",
    "        # Make backwards pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        # Accumulate training accuracy and loss\n",
    "        train_acc += calculate_accuracy(output.logits, label).item()\n",
    "        train_loss += loss.item()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop:\n",
    "    # This loop tests the trained network on validation dataset\n",
    "    # No weight updates here\n",
    "    # torch.no_grad() reduces memory usage when not training the network\n",
    "    model.eval() # Activate evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            data = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch[\"labels\"].to(device)\n",
    "            # Make forward pass with the trained model so far\n",
    "            output = model(data, attention_mask=attention_mask, labels=label)\n",
    "            # Accumulate validation accuracy and loss\n",
    "            valid_acc += calculate_accuracy(output.logits, label).item()\n",
    "            val_loss += output[0].item()\n",
    "        \n",
    "    # Take averages\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    val_loss /= len(validation_loader)\n",
    "    valid_acc /= len(validation_loader)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Epoch %d: train loss %.3f, train acc %.3f, val loss %.3f, val acc %.3f, seconds % .3f \" % (\n",
    "        epoch+1, train_loss, train_acc, val_loss, valid_acc, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58355913",
   "metadata": {},
   "source": [
    "### Looking at what's going on\n",
    "\n",
    "The fine-tuned BERT model is able to correctly classify the sentiment of the most of the records in the validation set. Let's observe in more detail how the sentences are tokenized and encoded. You can do this by picking one sentence as example to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "167b6555",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: An excellent resource for all scanner owners.  Seems to be up to date and all inclusive.  I highly recommend this product!\n",
      "Encoded Sentence: [101, 2019, 6581, 7692, 2005, 2035, 26221, 5608, 1012, 3849, 2000, 2022, 2039, 2000, 3058, 1998, 2035, 18678, 1012, 1045, 3811, 16755, 2023, 4031, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "st = val_texts[19]\n",
    "print(f\"Sentence: {st}\")\n",
    "tok = tokenizer(st, truncation=True, padding=True)\n",
    "print(f\"Encoded Sentence: {tok['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a7888",
   "metadata": {},
   "source": [
    "Print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a063fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mapped vocabulary is stored in tokenizer.vocab\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bca42a",
   "metadata": {},
   "source": [
    "Use the encoded sentence with the tokenizer to recover the original sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7838f99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'an', 'excellent', 'resource', 'for', 'all', 'scanner', 'owners', '.', 'seems', 'to', 'be', 'up', 'to', 'date', 'and', 'all', 'inclusive', '.', 'i', 'highly', 'recommend', 'this', 'product', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Methods convert_ids_to_tokens and convert_tokens_to_ids allow to see how sentences are tokenized\n",
    "print(tokenizer.convert_ids_to_tokens(tok[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41840ad",
   "metadata": {},
   "source": [
    "## Getting predictions on the test data\n",
    "\n",
    "After the model is trained, you can focus on getting test data to make predictions with. Do this by:\n",
    "- Reading and format the test dataset\n",
    "- Passing the test data to your trained model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb1979f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33276</td>\n",
       "      <td>I've been using greeting card software for wel...</td>\n",
       "      <td>Absolutely awful.</td>\n",
       "      <td>False</td>\n",
       "      <td>1300233600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20859</td>\n",
       "      <td>This version worked well for me, have upgraded...</td>\n",
       "      <td>Good for virtual machine on a mac</td>\n",
       "      <td>True</td>\n",
       "      <td>1448755200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63500</td>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1456963200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4950</td>\n",
       "      <td>I can assure you that any five star review was...</td>\n",
       "      <td>SCAM</td>\n",
       "      <td>False</td>\n",
       "      <td>1400803200</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26509</td>\n",
       "      <td>Overall the product really seems the same but ...</td>\n",
       "      <td>Has potential but many glitches and really the...</td>\n",
       "      <td>False</td>\n",
       "      <td>1419206400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  33276  I've been using greeting card software for wel...   \n",
       "1  20859  This version worked well for me, have upgraded...   \n",
       "2  63500                                             Great!   \n",
       "3   4950  I can assure you that any five star review was...   \n",
       "4  26509  Overall the product really seems the same but ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Absolutely awful.     False  1300233600   \n",
       "1                  Good for virtual machine on a mac      True  1448755200   \n",
       "2                                         Five Stars      True  1456963200   \n",
       "3                                               SCAM     False  1400803200   \n",
       "4  Has potential but many glitches and really the...     False  1419206400   \n",
       "\n",
       "   log_votes  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3   2.197225  \n",
       "4   0.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the test data (It doesn't have the isPositive label)\n",
    "df_test = pd.read_csv(\"data/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0accafc",
   "metadata": {},
   "source": [
    "Just as before, drop the rows that don't have the __reviewText__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfb677dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dd526",
   "metadata": {},
   "source": [
    "Making predictions will also take a long time with this model. To get results quickly, start by only making predictions with 15 datapoints from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d8ec7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts = df_test[\"reviewText\"].tolist()[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d2264bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts,\n",
    "                           truncation=True,\n",
    "                           padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4faf9",
   "metadata": {},
   "source": [
    "Create labels for the test dataset to pass zeros using `[0]*len(test_texts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96f828a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750865d3",
   "metadata": {},
   "source": [
    "Then, create a dataloader for the test set and record the corresponding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0145ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        predictions = torch.argmax(output.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee864ee",
   "metadata": {},
   "source": [
    "Finally, pick an example sentence and examine the prediction. Does the prediction look correct? \n",
    "\n",
    "Remember \n",
    "\n",
    "- 1->positive class \n",
    "- 0->negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2aa09fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I have been using this product for five or six years.  This purchase was my annual subscription renewal.  It has the features that I need, and seems to protect the three PC's that we have while using the internet.  I will be looking at the mobile apps for a tablet and smart phone.\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "k = 13\n",
    "print(f'Text: {test_texts[k]}')\n",
    "print(f'Prediction: {test_predictions[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ece07",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this lab you learned how to import a pre-trained Transformer model and fine-tune it for a specific task. Although you used a lighter version of the BERT model, these types of models tend to use large amounts of compute power. For that reason, you only worked with the first 2000 datapoints of the dataset. To see more general results, you need to spend more time training while using the whole dataset. \n",
    "\n",
    "## Next Lab: Reading and plotting images\n",
    "In the next lab you will learn how to read images and plot them as you start to learn about computer vision problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
